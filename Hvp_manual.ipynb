{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/niccoloajroldi/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_attr(obj, names):\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])\n",
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)\n",
    "\n",
    "def make_functional(mod):\n",
    "    orig_params = tuple(mod.parameters())\n",
    "    # Remove all the parameters in the model\n",
    "    names = []\n",
    "    for name, p in list(mod.named_parameters()):\n",
    "        del_attr(mod, name.split(\".\"))\n",
    "        names.append(name)\n",
    "    return orig_params, names\n",
    "\n",
    "def load_weights(mod, names, params):\n",
    "    for name, p in zip(names, params):\n",
    "        set_attr(mod, name.split(\".\"), p)\n",
    "\n",
    "def load_weights_from_optim(model, names, optimizer):\n",
    "    # create a list of params from optim\n",
    "    param_list = []\n",
    "    for g in optimizer.param_groups:\n",
    "        for p in g['params']:\n",
    "            if p.requires_grad:\n",
    "                param_list.append(p)\n",
    "    # load params into model\n",
    "    for name, p in zip(names, param_list):\n",
    "        set_attr(model, name.split(\".\"), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1, bias=False),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x).pow(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Computation of Hg, where H is the hessian, and g is the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([-1.2894e-08,  0.0000e+00, -2.3014e-09,  0.0000e+00])\n",
      "tensor([-7.9106e-10,  0.0000e+00, -1.4119e-10,  0.0000e+00])\n",
      "tensor([-6.8691e-08,  0.0000e+00, -1.2260e-08,  0.0000e+00])\n",
      "tensor([-4.5436e-06,  0.0000e+00, -8.1097e-07,  0.0000e+00])\n",
      "tensor([-2.1609e-06,  0.0000e+00, -3.8569e-07,  0.0000e+00])\n",
      "tensor([-6.6748e-08,  0.0000e+00, -1.1914e-08,  0.0000e+00])\n",
      "tensor([-0.0031,  0.0000, -0.0005,  0.0000])\n",
      "tensor([1.3738e-05, 0.0000e+00, 2.4521e-06, 0.0000e+00])\n",
      "tensor([3.5232e-04, 0.0000e+00, 6.2887e-05, 0.0000e+00])\n",
      "tensor([ 0.0000, -0.0038,  0.0000,  0.0129])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1996)\n",
    "\n",
    "model = mlp()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "n = 10\n",
    "x = torch.randn(n).reshape([n,1])\n",
    "y = 2. * x - 3* x.pow(2) + x.pow(3)\n",
    "\n",
    "for inputs, targets in zip(x,y):\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(create_graph=True) # needed to differentiate a second time\n",
    "    \n",
    "    # gp: inner prod between g and p\n",
    "    # in the iner prod g should require grad (difderentiate through it), p should not (constant)\n",
    "    gp = torch.tensor(0., requires_grad=False)\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.requires_grad:\n",
    "                gp = gp + torch.sum(torch.mul(p.grad, p.detach().clone())) # scalar prod\n",
    "    \n",
    "    # Hvp\n",
    "    Hz_lis = []\n",
    "    n_params = len(list(model.parameters()))\n",
    "    count = 0\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.requires_grad:\n",
    "                count += 1\n",
    "                retain = (count!=n_params)\n",
    "                Hp = torch.autograd.grad(gp, p, retain_graph=retain)[0]\n",
    "                Hz_lis.append(Hp)\n",
    "    \n",
    "    print(torch.cat([Hz_i.flatten() for Hz_i in Hz_lis]))\n",
    "    \n",
    "    # step\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
